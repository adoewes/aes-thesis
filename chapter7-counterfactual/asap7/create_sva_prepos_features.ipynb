{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f902a3-a7b0-4199-ba56-e300e9adf76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tool = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5906c555-64f1-4078-8918-31bdcf287fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm afraid for snakes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4ef6a5-da73-4891-8931-b9bba452ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = tool.check(sentence)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b23b8e-d69f-4ce3-99c6-d5d7616d5a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa8c093a-687f-48bc-bbf2-1405dea662aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_tool_python import LanguageTool\n",
    "\n",
    "def check_grammar(text):\n",
    "    tool = LanguageTool('en-US')\n",
    "    matches = tool.check(text)\n",
    "    \n",
    "    preposition_errors = []\n",
    "    for match in matches:\n",
    "        if 'preposition' in match.message.lower():\n",
    "            preposition_errors.append({\n",
    "                'error': match.message,\n",
    "                'suggestion': match.replacements,\n",
    "                'context': match.context\n",
    "            })\n",
    "    \n",
    "    return preposition_errors\n",
    "\n",
    "# Example usage\n",
    "errors = check_grammar(\"I‚Äôm afraid from snakes.\")\n",
    "for error in errors:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8ebc5b6-6ef9-49bf-89e8-9b907aa3fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sampled_prepositions(text, sample_size=5):\n",
    "    \"\"\"\n",
    "    Replace a sample of prepositions in the text with random prepositions to make the text wrong.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        nlp (spacy.language.Language): The loaded spaCy model.\n",
    "        sample_size (int): The number of prepositions to replace.\n",
    "    \n",
    "    Returns:\n",
    "        str: A JSON string with the original and replaced prepositions.\n",
    "    \"\"\"\n",
    "    # Define a list of prepositions to use as replacements\n",
    "    prepositions_list = ['about', 'accross', 'after', 'against', 'along', 'among', 'around', 'as', 'at', 'before', 'behind', 'beside', 'between', 'beyond', 'by', 'despite', 'down', 'during', 'except', 'for', 'in', 'inside', 'into', 'near', 'of', 'on', 'onto', 'outside', 'over', 'past', 'since', 'through', 'to', 'toward', 'under', 'underneath', 'until', 'up', 'upon', 'with', 'within', 'without']\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Identify all prepositions and their positions in sentences\n",
    "    prepositions = [(token.text, token.i, sent_idx) for sent_idx, sent in enumerate(doc.sents) for token in sent if token.pos_ == 'ADP']\n",
    "    \n",
    "    # Handle case where there are fewer prepositions than the sample size\n",
    "    sample_size = min(sample_size, len(prepositions))\n",
    "    \n",
    "    # Sample a subset of prepositions to replace\n",
    "    indices_to_replace = random.sample(range(len(prepositions)), sample_size)\n",
    "    \n",
    "    # Create replacements list\n",
    "    replacements = []\n",
    "    for idx in indices_to_replace:\n",
    "        original_preposition, token_index, sentence_num = prepositions[idx]\n",
    "        new_preposition = random.choice(prepositions_list)\n",
    "        \n",
    "        # Ensure the new preposition is different from the original\n",
    "        while new_preposition == original_preposition:\n",
    "            new_preposition = random.choice(prepositions_list)\n",
    "        \n",
    "        replacements.append({\n",
    "            \"original\": original_preposition,\n",
    "            \"error\": new_preposition,\n",
    "            \"sentence\": sentence_num + 1  # Adjusting to 1-based index for sentences\n",
    "        })\n",
    "    \n",
    "    return replacements #json.dumps(replacements, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4002b9e3-e106-49d7-ad2b-4bd60fdc244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6113292-cf53-413e-badf-293f91803c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': 'through', 'error': 'around', 'sentence': 2},\n",
       " {'original': 'at', 'error': 'of', 'sentence': 2},\n",
       " {'original': 'near', 'error': 'toward', 'sentence': 2},\n",
       " {'original': 'by', 'error': 'to', 'sentence': 1},\n",
       " {'original': 'on', 'error': 'under', 'sentence': 1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The cat jumped on the couch and slept by the window. She walked through the park and stopped at the bench near the fountain.\"\n",
    "\n",
    "replacements = replace_sampled_prepositions(text)\n",
    "replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6c862a-66c4-47e5-9cef-0fe58efb3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preposition_errors(text, replacements):\n",
    "    \"\"\"\n",
    "    Replace specified correct prepositions in the text with incorrect ones, based on sentence index.\n",
    "\n",
    "    Args:\n",
    "        text (str): Original input text.\n",
    "        replacements (list): List of dicts with keys: 'original', 'error', 'sentence'.\n",
    "\n",
    "    Returns:\n",
    "        str: Modified text with injected preposition errors.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    modified_sentences = []\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_text = sent.text\n",
    "        modified_sent = sent_text\n",
    "\n",
    "        # Find replacements for this sentence\n",
    "        sent_replacements = [r for r in replacements if r['sentence'] == i + 1]\n",
    "\n",
    "        # Replace prepositions one by one (cautiously)\n",
    "        for rep in sent_replacements:\n",
    "            pattern = rf'\\b{rep[\"original\"]}\\b'\n",
    "            modified_sent = re.sub(pattern, rep[\"error\"], modified_sent, count=1)\n",
    "\n",
    "        modified_sentences.append(modified_sent)\n",
    "\n",
    "    return \" \".join(modified_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0b424d-bd42-4c5c-8d70-04804fd4b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93ddac63-4a85-4cd7-b64d-b9eb072bb2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat jumped under the couch and slept to the window. She walked around the park and stopped of the bench toward the fountain.\n"
     ]
    }
   ],
   "source": [
    "corrupted_text = apply_preposition_errors(text, replacements)\n",
    "print(corrupted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3aee21e-af10-4908-949f-000666e55c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "def detect_preposition_errors_bert(text, top_k=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Detect potentially incorrect prepositions using BERT masked language modeling.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        top_k (int): Number of BERT suggestions to consider.\n",
    "        verbose (bool): If True, print debug info.\n",
    "\n",
    "    Returns:\n",
    "        List of detected issues.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent_tokens = [t.text for t in sent]\n",
    "        for token in sent:\n",
    "            if token.pos_ == \"ADP\" and token.is_alpha:\n",
    "                rel_index = token.i - sent.start  # index within sentence\n",
    "                original_prep = sent_tokens[rel_index]\n",
    "                masked_tokens = sent_tokens.copy()\n",
    "                masked_tokens[rel_index] = fill_mask.tokenizer.mask_token\n",
    "\n",
    "                masked_sentence = \" \".join(masked_tokens)\n",
    "                predictions = fill_mask(masked_sentence)\n",
    "                top_preds = [p[\"token_str\"].strip().lower() for p in predictions[:top_k]]\n",
    "\n",
    "                if verbose:\n",
    "                    print(f\"[DEBUG] '{original_prep}' vs {top_preds} in: {sent.text.strip()}\")\n",
    "\n",
    "                if original_prep.lower() not in top_preds:\n",
    "                    results.append({\n",
    "                        \"sentence\": sent.text.strip(),\n",
    "                        \"original\": original_prep,\n",
    "                        \"suggestions\": top_preds,\n",
    "                        \"masked_version\": masked_sentence,\n",
    "                        \"note\": f\"'{original_prep}' may be incorrect. Suggestions: {top_preds}\"\n",
    "                    })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48526524-e9ec-41a0-917e-78dfba09b28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj\n",
      "is AUX ROOT\n",
      "good ADJ acomp\n",
      "in ADP prep\n",
      "math NOUN pobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"She is good in math.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36e79625-3963-4a8a-afd7-6c9aa2707d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ‚ùå 'from' in: \"I‚Äôm afraid from snakes.\"\n",
      "  üëâ Suggested: ['of']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "def detect_preposition_errors(text, top_k=1):\n",
    "    results = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sent_tokens = [token.text for token in sent]\n",
    "        for i, token in enumerate(sent):\n",
    "            if token.pos_ == \"ADP\":\n",
    "                original = token.text\n",
    "                masked_tokens = sent_tokens.copy()\n",
    "                masked_tokens[i] = fill_mask.tokenizer.mask_token\n",
    "                masked_text = \" \".join(masked_tokens)\n",
    "\n",
    "                try:\n",
    "                    preds = fill_mask(masked_text)\n",
    "                    top_preds = [p[\"token_str\"].strip().lower() for p in preds[:top_k]]\n",
    "                    if original.lower() not in top_preds:\n",
    "                        results.append({\n",
    "                            \"sentence\": sent.text,\n",
    "                            \"original\": original,\n",
    "                            \"suggestions\": top_preds,\n",
    "                            \"masked\": masked_text\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with sentence: {masked_text}\")\n",
    "                    print(str(e))\n",
    "    return results\n",
    "\n",
    "# Run on text\n",
    "text = \"\"\"\n",
    "I‚Äôm afraid from snakes.\n",
    "\"\"\"\n",
    "\n",
    "errors = detect_preposition_errors(text)\n",
    "\n",
    "for e in errors:\n",
    "    print(f\"- ‚ùå '{e['original']}' in: \\\"{e['sentence'].strip()}\\\"\")\n",
    "    print(f\"  üëâ Suggested: {e['suggestions']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c91c8-50c0-4c69-af63-4131fe616364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d30ba-54e3-4fee-be35-90533ca5118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32db68-2669-4ee2-a608-f6a43e5c7f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38970c99-75ed-4876-b133-10888d4c8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He         POS: PRON  DEP: nsubj\n",
      "arrived    POS: VERB  DEP: ROOT\n",
      "to         POS: ADP  DEP: prep\n",
      "the        POS: DET  DEP: det\n",
      "station    POS: NOUN  DEP: pobj\n",
      "late       POS: ADV  DEP: advmod\n",
      ".          POS: PUNCT  DEP: punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"He arrived to the station late.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:10} POS: {token.pos_}  DEP: {token.dep_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5c580-1774-4670-ae82-c6db0780e715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
