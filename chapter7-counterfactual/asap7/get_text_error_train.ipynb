{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3b5430-0437-4ea1-aff8-2a5c59b5fd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from create_text_error import create_typos_from_text, replace_sampled_prepositions, break_sva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79324cd-0f3d-4af9-9323-7062e39cefcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19441</td>\n",
       "      <td>Patience is tough to achieve sometimes. But th...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18068</td>\n",
       "      <td>Patience a helpful trait to get you through li...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19000</td>\n",
       "      <td>There were many of times when I was patient. T...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18339</td>\n",
       "      <td>A time I was patiet was when I was wating for ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18297</td>\n",
       "      <td>Do you ever fell like your @CAPS1 is going to ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id                                              essay  domain1_score\n",
       "0     19441  Patience is tough to achieve sometimes. But th...             22\n",
       "1     18068  Patience a helpful trait to get you through li...             24\n",
       "2     19000  There were many of times when I was patient. T...             18\n",
       "3     18339  A time I was patiet was when I was wating for ...             24\n",
       "4     18297  Do you ever fell like your @CAPS1 is going to ...             21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ASAP7 Train Set.tsv', sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36bf31b-4f1a-48ce-a0d8-293bc31c1ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19441</td>\n",
       "      <td>Patience is tough to achieve sometimes. But th...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18068</td>\n",
       "      <td>Patience a helpful trait to get you through li...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19000</td>\n",
       "      <td>There were many of times when I was patient. T...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18339</td>\n",
       "      <td>A time I was patiet was when I was wating for ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18297</td>\n",
       "      <td>Do you ever fell like your @CAPS1 is going to ...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id                                              essay  domain1_score\n",
       "0     19441  Patience is tough to achieve sometimes. But th...             22\n",
       "1     18068  Patience a helpful trait to get you through li...             24\n",
       "2     19000  There were many of times when I was patient. T...             18\n",
       "3     18339  A time I was patiet was when I was wating for ...             24\n",
       "4     18297  Do you ever fell like your @CAPS1 is going to ...             21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample 100 data to get corrections from OpenAI\n",
    "data_100 = data[:100]\n",
    "data_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b6d674-91e8-4b76-8a1a-185fa2149f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test with one essay sample\n",
    "essay = data_100.iloc[20]['essay']\n",
    "\n",
    "def get_text_error(text):\n",
    "    typos = create_typos_from_text(text)\n",
    "    preps = replace_sampled_prepositions(text, 2)\n",
    "    svas = break_sva(text)\n",
    "    return typos + preps + svas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0fd55d1-7ef5-4d5c-ae01-371b58a469e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': 'patient', 'error': 'aptient', 'sentence': 1},\n",
       " {'original': 'got', 'error': 'gto', 'sentence': 2},\n",
       " {'original': 'thought', 'error': 'thoguht', 'sentence': 3},\n",
       " {'original': 'wait', 'error': 'awit', 'sentence': 4},\n",
       " {'original': 'patient', 'error': 'patinet', 'sentence': 5},\n",
       " {'original': 'with', 'error': 'about', 'sentence': 5},\n",
       " {'original': 'In', 'error': 'except', 'sentence': 3},\n",
       " {'original': 'know', 'error': 'knew', 'sentence': 14},\n",
       " {'original': 'remember', 'error': 'remembers', 'sentence': 1},\n",
       " {'original': 'ask', 'error': 'asked', 'sentence': 7},\n",
       " {'original': 'work', 'error': 'wrought', 'sentence': 13},\n",
       " {'original': 'have', 'error': 'had', 'sentence': 9}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_error(essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa3cff3-4670-473a-af59-0debfcd53522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing essay 1\n",
      "Finished processing essay 2\n",
      "Finished processing essay 3\n",
      "Finished processing essay 4\n",
      "Finished processing essay 5\n",
      "Finished processing essay 6\n",
      "Finished processing essay 7\n",
      "Finished processing essay 8\n",
      "Finished processing essay 9\n",
      "Finished processing essay 10\n",
      "Finished processing essay 11\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 12\n",
      "Finished processing essay 13\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 14\n",
      "Finished processing essay 15\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 16\n",
      "Finished processing essay 17\n",
      "Finished processing essay 18\n",
      "Finished processing essay 19\n",
      "Finished processing essay 20\n",
      "Finished processing essay 21\n",
      "Finished processing essay 22\n",
      "Finished processing essay 23\n",
      "[Warning] Only 2 errors could be generated (requested 5).\n",
      "Finished processing essay 24\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 25\n",
      "Finished processing essay 26\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 27\n",
      "Finished processing essay 28\n",
      "Finished processing essay 29\n",
      "[Warning] Only 2 errors could be generated (requested 5).\n",
      "Finished processing essay 30\n",
      "Finished processing essay 31\n",
      "Finished processing essay 32\n",
      "Finished processing essay 33\n",
      "Finished processing essay 34\n",
      "Finished processing essay 35\n",
      "Finished processing essay 36\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 37\n",
      "Finished processing essay 38\n",
      "Finished processing essay 39\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 40\n",
      "Finished processing essay 41\n",
      "Finished processing essay 42\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 43\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 44\n",
      "[Warning] Only 0 errors could be generated (requested 5).\n",
      "Finished processing essay 45\n",
      "Finished processing essay 46\n",
      "Finished processing essay 47\n",
      "Finished processing essay 48\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 49\n",
      "[Warning] Only 0 errors could be generated (requested 5).\n",
      "Finished processing essay 50\n",
      "Finished processing essay 51\n",
      "[Warning] Only 2 errors could be generated (requested 5).\n",
      "Finished processing essay 52\n",
      "Finished processing essay 53\n",
      "[Warning] Only 2 errors could be generated (requested 5).\n",
      "Finished processing essay 54\n",
      "Finished processing essay 55\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 56\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 57\n",
      "Finished processing essay 58\n",
      "Finished processing essay 59\n",
      "Finished processing essay 60\n",
      "Finished processing essay 61\n",
      "Finished processing essay 62\n",
      "Finished processing essay 63\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 64\n",
      "Finished processing essay 65\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 66\n",
      "Finished processing essay 67\n",
      "Finished processing essay 68\n",
      "Finished processing essay 69\n",
      "Finished processing essay 70\n",
      "[Warning] Only 0 errors could be generated (requested 5).\n",
      "Finished processing essay 71\n",
      "Finished processing essay 72\n",
      "[Warning] Only 2 errors could be generated (requested 5).\n",
      "Finished processing essay 73\n",
      "Finished processing essay 74\n",
      "Finished processing essay 75\n",
      "Finished processing essay 76\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 77\n",
      "Finished processing essay 78\n",
      "Finished processing essay 79\n",
      "Finished processing essay 80\n",
      "Finished processing essay 81\n",
      "Finished processing essay 82\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 83\n",
      "Finished processing essay 84\n",
      "Finished processing essay 85\n",
      "Finished processing essay 86\n",
      "Finished processing essay 87\n",
      "Finished processing essay 88\n",
      "Finished processing essay 89\n",
      "[Warning] Only 4 errors could be generated (requested 5).\n",
      "Finished processing essay 90\n",
      "Finished processing essay 91\n",
      "Finished processing essay 92\n",
      "Finished processing essay 93\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 94\n",
      "Finished processing essay 95\n",
      "Finished processing essay 96\n",
      "Finished processing essay 97\n",
      "[Warning] Only 1 errors could be generated (requested 5).\n",
      "Finished processing essay 98\n",
      "[Warning] Only 3 errors could be generated (requested 5).\n",
      "Finished processing essay 99\n",
      "Finished processing essay 100\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the JSON responses\n",
    "error_lists = []\n",
    "\n",
    "ctr = 0\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in data_100.iterrows():\n",
    "    try:\n",
    "        # Extract the essay text\n",
    "        essay = row['essay']\n",
    "        # Get the error response\n",
    "        error_list = get_text_error(essay)\n",
    "        # Append the result to the list\n",
    "        error_lists.append(error_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        error_lists.append(None)  # Append None or an empty dict if an error occurs\n",
    "    ctr = ctr + 1\n",
    "    print(f\"Finished processing essay {ctr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cac02fb-163c-413a-b275-a25d93aac88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86591cea-c686-45ce-81ec-7c7c58127733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "error_array = np.array(error_lists, dtype=object)\n",
    "print(error_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abb4fff6-9885-4c57-88d0-e91ffaeedf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/error_lists_train_100']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(error_array, 'files/error_lists_train_100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0850ef-a93c-4023-aba1-7b91da125eca",
   "metadata": {},
   "source": [
    "#### Get scores from 100 sampled essay (train set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffebc0f9-cb8a-4bff-a843-f50363b9b721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/scores_essay_100_train_ori']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect scores from sampled testing set\n",
    "scores = data_100['domain1_score']\n",
    "joblib.dump(scores, \"files/scores_essay_100_train_ori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734e35a-4d95-42ee-9255-ceded32274ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6971f70-94d3-4ea5-9e61-8cc3c7537876",
   "metadata": {},
   "source": [
    "#### Try Grammar Error : Subject-Verb disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5466a932-560f-4c98-8a33-61a839f6ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "import pyinflect   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b45cbad-d9cb-44de-912b-9f19773aae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences_with_numbers(text):\n",
    "    doc = nlp(text)\n",
    "    for i, sent in enumerate(doc.sents, start=1):\n",
    "        print(f\"Sentence {i}: {sent.text}\")\n",
    "\n",
    "def load_spacy_model(model_name=\"en_core_web_sm\"):\n",
    "    \"\"\"Load the spaCy model.\"\"\"\n",
    "    return spacy.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe5e7198-f4b0-4a6a-8a3a-c236ef0b7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model once\n",
    "nlp = load_spacy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78285419-f7ee-41e5-9940-e44397c88a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal irregular fallback map for very common verbs\n",
    "IRREGULAR = {\n",
    "    (\"go\", \"VBZ\"): \"goes\",\n",
    "    (\"go\", \"VBD\"): \"went\",\n",
    "    (\"go\", \"VBN\"): \"gone\",\n",
    "    (\"be\", \"VBZ\"): \"is\",\n",
    "    (\"be\", \"VBP\"): \"are\",\n",
    "    (\"be\", \"VBD\"): \"was\",\n",
    "    (\"be\", \"VBN\"): \"been\",\n",
    "    (\"have\", \"VBZ\"): \"has\",\n",
    "    (\"have\", \"VBD\"): \"had\",\n",
    "    (\"do\", \"VBZ\"): \"does\",\n",
    "    (\"do\", \"VBD\"): \"did\",\n",
    "}\n",
    "\n",
    "def inflect_form(lemma, target_tag):\n",
    "    \"\"\"\n",
    "    Try pyinflect first; if that fails, fall back to IRREGULAR map\n",
    "    then to naive suffix rules.\n",
    "    \"\"\"\n",
    "    # pyinflect wants a spaCy token; make a dummy one\n",
    "    dummy = nlp(lemma)[0]\n",
    "    inflected = dummy._.inflect(target_tag)\n",
    "    if inflected:\n",
    "        return inflected\n",
    "    \n",
    "    # fallback to irregular dictionary\n",
    "    if (lemma, target_tag) in IRREGULAR:\n",
    "        return IRREGULAR[(lemma, target_tag)]\n",
    "    \n",
    "    # crude suffix fall-back (only for regular verbs)\n",
    "    if target_tag == \"VBZ\":\n",
    "        return lemma + \"s\"\n",
    "    if target_tag == \"VBD\":\n",
    "        return lemma + \"ed\"\n",
    "    if target_tag == \"VBP\":\n",
    "        return lemma                    # base already\n",
    "    return None                         # give up\n",
    "\n",
    "    \n",
    "def break_sva(text, n_err=5, nlp=nlp):\n",
    "    \"\"\"\n",
    "    Inject exactly `n_err` subjectâ€“verb agreement errors using correct inflections.\n",
    "    Prints a warning if fewer than `n_err` errors are possible.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "    else:\n",
    "        doc = text\n",
    "\n",
    "    # Collect all candidate (verb, sentence) pairs\n",
    "    candidates = []\n",
    "    for sent_no, sent in enumerate(doc.sents, start=1):\n",
    "        for tok in sent:\n",
    "            if tok.dep_ == \"ROOT\" and tok.pos_ == \"VERB\":\n",
    "                if any(w.dep_ in (\"nsubj\", \"nsubjpass\") for w in tok.lefts):\n",
    "                    candidates.append((tok, sent_no))\n",
    "\n",
    "    random.shuffle(candidates)\n",
    "    errors = []\n",
    "\n",
    "    for tok, sent_no in candidates:\n",
    "        if len(errors) >= n_err:\n",
    "            break\n",
    "\n",
    "        lemma = tok.lemma_.lower()\n",
    "        tag   = tok.tag_\n",
    "\n",
    "        if tag == \"VBZ\":\n",
    "            target_tag = random.choice([\"VBP\", \"VBD\"])\n",
    "        elif tag == \"VBP\":\n",
    "            target_tag = random.choice([\"VBZ\", \"VBD\"])\n",
    "        elif tag == \"VBD\":\n",
    "            target_tag = random.choice([\"VBZ\", \"VBP\"])\n",
    "        elif tag in (\"VBG\", \"VBN\"):\n",
    "            target_tag = random.choice([\"VBZ\", \"VBP\"])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        wrong_form = inflect_form(lemma, target_tag)\n",
    "        if not wrong_form:\n",
    "            continue\n",
    "        if wrong_form.lower() == tok.text.lower():\n",
    "            continue\n",
    "\n",
    "        errors.append({\n",
    "            \"original\": tok.text,\n",
    "            \"error\": wrong_form,\n",
    "            \"sentence\": sent_no\n",
    "        })\n",
    "\n",
    "    # Fallback warning\n",
    "    if len(errors) < n_err:\n",
    "        print(f\"[Warning] Only {len(errors)} errors could be generated (requested {n_err}).\")\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "946166f8-0985-4f00-ad3e-06a8de707dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People using computers is a good way for them to talk with distant relatives or friends, learn keyboarding skills, and research about far-away places. First, the wonder ful technology that computers have is amazing. They can surf the web in milli-seconds. This benefits people because if you were doing a research paper on china, many search engines let you surf the web at blazing fast speed, and in a little time, you could have all the information you needed. Scientists say that @PERCENT1 of computer-users have fast internet, and they reccommend it to other, first-time buyers. The computer that I use is a unique computer that was customizable to my liking. I like playing games and editing videos, so I got an outstanding graphics car that has outstanding speeds for rendering videos. This let me play games and edit videos, and my @NUM1 gb hard drive is capable of storing all my programs and projects, for future use. Doing many things on the computer can really help your hand-eye coordination. In fact, about @PERCENT2 of computer say that over a using a computer. they were to type @PERCENT3 quicker that they could before. Thats a very good number, but the average person woh types could write in @NUM2 in I have. I can type a that big in about @NUM3 minutes because it have been typing in other @NUM3. Computers are a big part of my life and I still have time to do other things with my family, and friends. The last thing that computers help us people with chatting with friends on websites, like facebook, myspace, and @ORGANIZATION1. They all let you share with all of your friends, and of you needed to, you could chat with far away friends or relatives. This helps because if you haven't talked to your cousin in a while, you can check up with him and talk about things. I have a causing who graduated college and is now teaching in @LOCATION1, and when our gets together, we webchat with him on @ORGANIZATION1 and it is helpful because we haven't seen him in @NUM5 years, and we all wanted to see what he was up to. He was also able to send us pictures, which helped because we all missed his face. So, blazing fast speed to chatting with distant relatives. These are key factors that show us that you can have fun on computers and still have time for family and friends.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data_100.iloc[1]['essay']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56c66a5a-1664-4027-a402-e044b05ce091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'original': 'have', 'error': 'had', 'sentence': 18},\n",
       " {'original': 'helps', 'error': 'help', 'sentence': 17},\n",
       " {'original': 'let', 'error': 'lets', 'sentence': 8},\n",
       " {'original': 'got', 'error': 'get', 'sentence': 7},\n",
       " {'original': 'say', 'error': 'says', 'sentence': 5}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "break_sva(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6db7ba6-ae03-481d-b271-408fcaf4a8ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Computers and the @CAPS1 were a technological break through.\n",
      "Sentence 2: It exposed to the average world, things that were never thought possitive.\n",
      "Sentence 3: But as these things advanced over the years, they've become an addiction so bad of an addiction its begun to threaten peoples lives I've been given a choice to s'de with the addicting computers, or to offose them.\n",
      "Sentence 4: The only clear choice is to offose.\n",
      "Sentence 5: First off, computers have caused the world a decrease in exercise.\n",
      "Sentence 6: Studies show @NUM1 out of @NUM2 people who use a computer, do not exercise with less exercise throughout the world, nations are becoming more over weight.\n",
      "Sentence 7: This is a huge problem in the united states computers are main cause to why the @CAPS2.S is over weight and unhealthy by cutting down computers use, we can get our world back into great shape we can bring exercise and health back.\n",
      "Sentence 8: Nextly, I'm sure you've all heard of online predators.\n",
      "Sentence 9: It's scary just to think about, well as computer technology increased online predators numbers went up.\n",
      "Sentence 10: I remember a couple of years ago, I was watching the news and a story came a normal teenage girl, being killed by someone she meton myspace.\n",
      "Sentence 11: things like this still go on, and the rate at which they happen are in creasing.\n",
      "Sentence 12: by putting people on computers you're putting then at risk of death this is an extreme problem computers have caused.\n",
      "Sentence 13: Thus is the last efferct computers have, out of many that I'm going to state time on the computer, is time taken away from family and friends.\n",
      "Sentence 14: This can ruin relationship in and outside the family.\n",
      "Sentence 15: Now I'm sure you now many people have become extremely addicted to online games.\n",
      "Sentence 16: the more they play these games the more they pull away from everyone they knew and loved computers are a leading cause in disfunctional families.\n",
      "Sentence 17: they steal the user away from the outside word.\n",
      "Sentence 18: these people need to get their families and friends back.\n",
      "Sentence 19: You have to act now, before its too late and computers have over taken the world.\n",
      "Sentence 20: If you know anyone who has fallen prey to a computer addiction, do what you can to help get them back we need to cutt back on any kind of computer use fast.\n",
      "Sentence 21: Hurry, it's now or never.\n"
     ]
    }
   ],
   "source": [
    "print_sentences_with_numbers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856b796-588e-40de-b7c8-bfe323f74772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
