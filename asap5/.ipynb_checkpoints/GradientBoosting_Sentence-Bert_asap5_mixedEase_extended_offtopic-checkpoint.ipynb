{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('essay_ease10_sbert768_simbow_langerr_780_normalized_asap5')\n",
    "x_off = joblib.load('essay_asap5_780_with350offtopic')\n",
    "y = joblib.load('score_asap5')\n",
    "y_off = joblib.load('score_asap5_with350offtopic')\n",
    "off = joblib.load('essay_350_offtopic_780_except5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1805, 780)\n",
      "(2155, 780)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1805,)\n",
      "(2155,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 780)\n"
     ]
    }
   ],
   "source": [
    "print(off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len feature names:  780\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names_extended():\n",
    "    ease_feats = ['Answer Length', 'Word Counts', 'Average Word Length', 'Good n-gram', 'Prompt Overlap',\n",
    "              'Prompt Overlap (synonyms)', 'Punctuation Counts', 'Spelling Error', 'Unique Words', 'Prompt Similarity SBert']\n",
    "\n",
    "    sbert_feats = []\n",
    "    sbert_dim = 768\n",
    "\n",
    "    for i in range(0, sbert_dim):\n",
    "    \tfname = \"sbert_\" + str(i) \n",
    "    \tsbert_feats.append(fname)\n",
    "    \n",
    "    prompt_similarity_bow = [\"Prompt Similarity BOW\"]\n",
    "    lang_error = [\"Language Error\"]\n",
    "    \n",
    "    feature_names = ease_feats + prompt_similarity_bow + lang_error + sbert_feats \n",
    "\n",
    "    print(\"len feature names: \", len(feature_names))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names_extended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
    "                colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original + off topic data (2155 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.8967100229533282\n",
      "Acc :  0.7215777262180975\n",
      "len all :  431\n",
      "Qwk original :  0.8142253149696728\n",
      "Acc original :  0.6927374301675978\n",
      "len ori :  358\n",
      "Acc off topic :  0.863013698630137\n",
      "len off :  73\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.8937118527893929\n",
      "Acc :  0.7122969837587007\n",
      "len all :  431\n",
      "Qwk original :  0.7995202961217021\n",
      "Acc original :  0.6721311475409836\n",
      "len ori :  366\n",
      "Acc off topic :  0.9384615384615385\n",
      "len off :  65\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.8861467706838719\n",
      "Acc :  0.691415313225058\n",
      "len all :  431\n",
      "Qwk original :  0.8128063272683697\n",
      "Acc original :  0.6720867208672087\n",
      "len ori :  369\n",
      "Acc off topic :  0.8064516129032258\n",
      "len off :  62\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.8816255047154332\n",
      "Acc :  0.6844547563805105\n",
      "len all :  431\n",
      "Qwk original :  0.7567683870317979\n",
      "Acc original :  0.637883008356546\n",
      "len ori :  359\n",
      "Acc off topic :  0.9166666666666666\n",
      "len off :  72\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.8997434456388306\n",
      "Acc :  0.6960556844547564\n",
      "len all :  431\n",
      "Qwk original :  0.7923955863408078\n",
      "Acc original :  0.660056657223796\n",
      "len ori :  353\n",
      "Acc off topic :  0.8589743589743589\n",
      "len off :  78\n",
      "\n",
      "Mean QWK :  0.8915875193561714\n",
      "\n",
      "Mean QWK Original :  0.7951431823464701\n",
      "\n",
      "Mean Accuracy :  0.7011600928074245\n",
      "\n",
      "Mean Accuracy Original :  0.6669789928312264\n",
      "\n",
      "Mean Accuracy Off Topic :  0.8767135751271853\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "qwk_scores_ori = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_ori = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_ori = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_ori = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x_off, y_off):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x_off[train_index], x_off[test_index], y_off[train_index], y_off[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ALL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "\n",
    "    # PREDICT AND EVALUATE ONLY ORIGINAL ESSAY\n",
    "    test_index_ori = [a for a in test_index if a < 1805]\n",
    "    x_test_ori = x_off[test_index_ori]\n",
    "    y_test_ori = y_off[test_index_ori]\n",
    "    predict_ori = model2.predict(x_test_ori)\n",
    "    predict_ori = np.round(predict_ori)\n",
    "    pred_labels_ori.extend(predict_ori)\n",
    "    \n",
    "    result_qwk_ori = quadratic_weighted_kappa(y_test_ori, predict_ori)\n",
    "    print(\"Qwk original : \", result_qwk_ori)\n",
    "    qwk_scores_ori.append(result_qwk_ori)\n",
    "    \n",
    "    result_acc_ori = accuracy_score(y_test_ori, predict_ori)\n",
    "    print(\"Acc original : \", result_acc_ori)\n",
    "    acc_scores_ori.append(result_acc_ori)\n",
    "    \n",
    "    print(\"len ori : \", len(test_index_ori))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY OFF-TOPIC ESSAY\n",
    "    test_index_off = [a for a in test_index if a > 1804]\n",
    "    x_test_off = x_off[test_index_off]\n",
    "    y_test_off = y_off[test_index_off]\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(test_index_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Original : \", np.mean(qwk_scores_ori))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Original : \", np.mean(acc_scores_ori))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also check for minus score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2.0: 693, 3.0: 582, 1.0: 301, 4.0: 222, -0.0: 7})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-0.0: 307, 1.0: 41, 2.0: 1, -1.0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i < 1 for i in pred_labels_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy Off Topic :  0.88\n"
     ]
    }
   ],
   "source": [
    "# SO the Accuracies is 194 / 350\n",
    "print(\"\\nMean Accuracy Off Topic : \", sum(i < 1 for i in pred_labels_off) / len(pred_labels_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original data (1805 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.7838152757725856\n",
      "Acc :  0.7008310249307479\n",
      "len all :  361\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.7969935540186665\n",
      "Acc :  0.6703601108033241\n",
      "len all :  361\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.8273125645997591\n",
      "Acc :  0.6980609418282548\n",
      "len all :  361\n",
      "Acc off topic :  0.02\n",
      "len off :  350\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.7991864015252848\n",
      "Acc :  0.6537396121883656\n",
      "len all :  361\n",
      "Acc off topic :  0.002857142857142857\n",
      "len off :  350\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.8252928789114105\n",
      "Acc :  0.6925207756232687\n",
      "len all :  361\n",
      "Acc off topic :  0.005714285714285714\n",
      "len off :  350\n",
      "\n",
      "Mean QWK :  0.8065201349655412\n",
      "\n",
      "Mean Accuracy :  0.6831024930747922\n",
      "\n",
      "Mean Accuracy Off Topic :  0.005714285714285714\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ORIGINAL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY offBERISH ESSAY\n",
    "    x_test_off = off\n",
    "    y_test_off = np.zeros(350)\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(x_test_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2.0: 693, 3.0: 582, 1.0: 301, 4.0: 222, -0.0: 7})\n",
      "Counter({2.0: 871, 1.0: 452, 3.0: 416, 0.0: 10, 4.0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))\n",
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_asap6_extended_780_normalized')\n",
    "\n",
    "d_off = xgboost.DMatrix(off, feature_names=feature_names)\n",
    "pred = model.predict(d_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 3., 1., 2., 1., 1., 1., 2., 3., 2., 2., 1., 2., 2., 1.,\n",
       "       2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 3., 1., 3., 2., 0., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 3., 2., 3., 2., 1., 2., 3., 2., 2., 2., 2., 3., 1., 2., 2., 3.,\n",
       "       1., 3., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 1., 1., 3., 2., 2., 3., 3., 2., 1., 2., 3.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 3., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 2., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 1., 2., 1., 2., 2., 2.,\n",
       "       2., 1., 1., 2., 1., 2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.round(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 169, 2.0: 142, 3.0: 18, 0.0: 21})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 54.29%\n"
     ]
    }
   ],
   "source": [
    "pred_failed = [a for a in pred if a < 2]\n",
    "acc = (len(pred_failed) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 6.00%\n"
     ]
    }
   ],
   "source": [
    "pred_zero = [a for a in pred if a == 0]\n",
    "acc = (len(pred_zero) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
