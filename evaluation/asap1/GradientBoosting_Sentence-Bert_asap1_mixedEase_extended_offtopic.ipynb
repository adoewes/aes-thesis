{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('essay_ease10_sbert768_simbow_langerr_780_normalized_asap1')\n",
    "x_off = joblib.load('essay_asap1_780_with350offtopic')\n",
    "y = joblib.load('score_asap1')\n",
    "y_off = joblib.load('score_asap1_with350offtopic')\n",
    "off = joblib.load('essay_350_offtopic_780_except1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 780)\n",
      "(2133, 780)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783,)\n",
      "(2133,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 780)\n"
     ]
    }
   ],
   "source": [
    "print(off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len feature names:  780\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names_extended():\n",
    "    ease_feats = ['Answer Length', 'Word Counts', 'Average Word Length', 'Good n-gram', 'Prompt Overlap',\n",
    "              'Prompt Overlap (synonyms)', 'Punctuation Counts', 'Spelling Error', 'Unique Words', 'Prompt Similarity SBert']\n",
    "\n",
    "    sbert_feats = []\n",
    "    sbert_dim = 768\n",
    "\n",
    "    for i in range(0, sbert_dim):\n",
    "    \tfname = \"sbert_\" + str(i) \n",
    "    \tsbert_feats.append(fname)\n",
    "    \n",
    "    prompt_similarity_bow = [\"Prompt Similarity BOW\"]\n",
    "    lang_error = [\"Language Error\"]\n",
    "    \n",
    "    feature_names = ease_feats + prompt_similarity_bow + lang_error + sbert_feats \n",
    "\n",
    "    print(\"len feature names: \", len(feature_names))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names_extended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
    "                colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original + off topic data (2133 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.9510381998088276\n",
      "Acc :  0.5058548009367682\n",
      "len all :  427\n",
      "Qwk original :  0.7671536418003388\n",
      "Acc original :  0.5100286532951289\n",
      "len ori :  349\n",
      "Acc off topic :  0.48717948717948717\n",
      "len off :  78\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.9320766722341526\n",
      "Acc :  0.4637002341920375\n",
      "len all :  427\n",
      "Qwk original :  0.6753393900334712\n",
      "Acc original :  0.4401114206128134\n",
      "len ori :  359\n",
      "Acc off topic :  0.5882352941176471\n",
      "len off :  68\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.9189463633165578\n",
      "Acc :  0.43559718969555034\n",
      "len all :  427\n",
      "Qwk original :  0.6604223734205178\n",
      "Acc original :  0.42587601078167114\n",
      "len ori :  371\n",
      "Acc off topic :  0.5\n",
      "len off :  56\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.9395171920884239\n",
      "Acc :  0.4694835680751174\n",
      "len all :  426\n",
      "Qwk original :  0.7024718708617238\n",
      "Acc original :  0.4456824512534819\n",
      "len ori :  359\n",
      "Acc off topic :  0.5970149253731343\n",
      "len off :  67\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.9431422384959174\n",
      "Acc :  0.43661971830985913\n",
      "len all :  426\n",
      "Qwk original :  0.7101208754317319\n",
      "Acc original :  0.43478260869565216\n",
      "len ori :  345\n",
      "Acc off topic :  0.4444444444444444\n",
      "len off :  81\n",
      "\n",
      "Mean QWK :  0.9369441331887758\n",
      "\n",
      "Mean QWK Original :  0.7031016303095566\n",
      "\n",
      "Mean Accuracy :  0.46225110224186644\n",
      "\n",
      "Mean Accuracy Original :  0.4512962289277495\n",
      "\n",
      "Mean Accuracy Off Topic :  0.5233748302229426\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "qwk_scores_ori = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_ori = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_ori = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_ori = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x_off, y_off):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x_off[train_index], x_off[test_index], y_off[train_index], y_off[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ALL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "\n",
    "    # PREDICT AND EVALUATE ONLY ORIGINAL ESSAY\n",
    "    test_index_ori = [a for a in test_index if a < 1783]\n",
    "    x_test_ori = x_off[test_index_ori]\n",
    "    y_test_ori = y_off[test_index_ori]\n",
    "    predict_ori = model2.predict(x_test_ori)\n",
    "    predict_ori = np.round(predict_ori)\n",
    "    pred_labels_ori.extend(predict_ori)\n",
    "    \n",
    "    result_qwk_ori = quadratic_weighted_kappa(y_test_ori, predict_ori)\n",
    "    print(\"Qwk original : \", result_qwk_ori)\n",
    "    qwk_scores_ori.append(result_qwk_ori)\n",
    "    \n",
    "    result_acc_ori = accuracy_score(y_test_ori, predict_ori)\n",
    "    print(\"Acc original : \", result_acc_ori)\n",
    "    acc_scores_ori.append(result_acc_ori)\n",
    "    \n",
    "    print(\"len ori : \", len(test_index_ori))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY OFF-TOPIC ESSAY\n",
    "    test_index_off = [a for a in test_index if a > 1782]\n",
    "    x_test_off = x_off[test_index_off]\n",
    "    y_test_off = y_off[test_index_off]\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(test_index_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Original : \", np.mean(qwk_scores_ori))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Original : \", np.mean(acc_scores_ori))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also check for minus score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({8.0: 538, 9.0: 479, 10.0: 279, 7.0: 228, 6.0: 114, 11.0: 101, 5.0: 29, 3.0: 8, 4.0: 3, 12.0: 2, 2.0: 2})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 182, 1.0: 92, 2.0: 40, 3.0: 16, -1.0: 12, 4.0: 3, 5.0: 3, 6.0: 2})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i < 3 for i in pred_labels_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy Off Topic :  0.9314285714285714\n"
     ]
    }
   ],
   "source": [
    "# SO the Accuracies is 194 / 350\n",
    "print(\"\\nMean Accuracy Off Topic : \", sum(i < 3 for i in pred_labels_off) / len(pred_labels_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original data (1783 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.8082407525645937\n",
      "Acc :  0.484593837535014\n",
      "len all :  357\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.7596110180240844\n",
      "Acc :  0.4677871148459384\n",
      "len all :  357\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.7823694002942279\n",
      "Acc :  0.48179271708683474\n",
      "len all :  357\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.7724125555173393\n",
      "Acc :  0.5365168539325843\n",
      "len all :  356\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.7906373334375365\n",
      "Acc :  0.4859550561797753\n",
      "len all :  356\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Mean QWK :  0.7826542119675564\n",
      "\n",
      "Mean Accuracy :  0.4913291159160293\n",
      "\n",
      "Mean Accuracy Off Topic :  0.0\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ORIGINAL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY offBERISH ESSAY\n",
    "    x_test_off = off\n",
    "    y_test_off = np.zeros(350)\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(x_test_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({8.0: 538, 9.0: 479, 10.0: 279, 7.0: 228, 6.0: 114, 11.0: 101, 5.0: 29, 3.0: 8, 4.0: 3, 12.0: 2, 2.0: 2})\n",
      "Counter({7.0: 542, 6.0: 445, 8.0: 300, 10.0: 167, 9.0: 164, 5.0: 79, 4.0: 28, 11.0: 15, 3.0: 10})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))\n",
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_asap6_extended_780_normalized')\n",
    "\n",
    "d_off = xgboost.DMatrix(off, feature_names=feature_names)\n",
    "pred = model.predict(d_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 3., 1., 2., 1., 1., 1., 2., 3., 2., 2., 1., 2., 2., 1.,\n",
       "       2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 3., 1., 3., 2., 0., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 3., 2., 3., 2., 1., 2., 3., 2., 2., 2., 2., 3., 1., 2., 2., 3.,\n",
       "       1., 3., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 1., 1., 3., 2., 2., 3., 3., 2., 1., 2., 3.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 3., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 2., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 1., 2., 1., 2., 2., 2.,\n",
       "       2., 1., 1., 2., 1., 2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.round(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 169, 2.0: 142, 3.0: 18, 0.0: 21})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 54.29%\n"
     ]
    }
   ],
   "source": [
    "pred_failed = [a for a in pred if a < 2]\n",
    "acc = (len(pred_failed) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 6.00%\n"
     ]
    }
   ],
   "source": [
    "pred_zero = [a for a in pred if a == 0]\n",
    "acc = (len(pred_zero) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
