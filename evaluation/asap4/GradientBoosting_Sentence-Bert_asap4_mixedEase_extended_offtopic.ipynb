{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('essay_ease10_sbert768_simbow_langerr_780_normalized_asap4')\n",
    "x_off = joblib.load('essay_asap4_780_with350offtopic')\n",
    "y = joblib.load('score_asap4')\n",
    "y_off = joblib.load('score_asap4_with350offtopic')\n",
    "off = joblib.load('essay_350_offtopic_780_except4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1772, 780)\n",
      "(2122, 780)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1772,)\n",
      "(2122,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 780)\n"
     ]
    }
   ],
   "source": [
    "print(off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len feature names:  780\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names_extended():\n",
    "    ease_feats = ['Answer Length', 'Word Counts', 'Average Word Length', 'Good n-gram', 'Prompt Overlap',\n",
    "              'Prompt Overlap (synonyms)', 'Punctuation Counts', 'Spelling Error', 'Unique Words', 'Prompt Similarity SBert']\n",
    "\n",
    "    sbert_feats = []\n",
    "    sbert_dim = 768\n",
    "\n",
    "    for i in range(0, sbert_dim):\n",
    "    \tfname = \"sbert_\" + str(i) \n",
    "    \tsbert_feats.append(fname)\n",
    "    \n",
    "    prompt_similarity_bow = [\"Prompt Similarity BOW\"]\n",
    "    lang_error = [\"Language Error\"]\n",
    "    \n",
    "    feature_names = ease_feats + prompt_similarity_bow + lang_error + sbert_feats \n",
    "\n",
    "    print(\"len feature names: \", len(feature_names))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names_extended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
    "                colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.05,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original + off topic data (2122 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.8165710296643677\n",
      "Acc :  0.7105882352941176\n",
      "len all :  425\n",
      "Qwk original :  0.7482342825557352\n",
      "Acc original :  0.6598837209302325\n",
      "len ori :  344\n",
      "Acc off topic :  0.9259259259259259\n",
      "len off :  81\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.8215936945003008\n",
      "Acc :  0.6847058823529412\n",
      "len all :  425\n",
      "Qwk original :  0.7608976398585039\n",
      "Acc original :  0.6410958904109589\n",
      "len ori :  365\n",
      "Acc off topic :  0.95\n",
      "len off :  60\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.8353482323475581\n",
      "Acc :  0.7240566037735849\n",
      "len all :  424\n",
      "Qwk original :  0.7881180743085948\n",
      "Acc original :  0.6948228882833788\n",
      "len ori :  367\n",
      "Acc off topic :  0.9122807017543859\n",
      "len off :  57\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.8236505086774386\n",
      "Acc :  0.7004716981132075\n",
      "len all :  424\n",
      "Qwk original :  0.7681376805873529\n",
      "Acc original :  0.6629213483146067\n",
      "len ori :  356\n",
      "Acc off topic :  0.8970588235294118\n",
      "len off :  68\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.8421453568626103\n",
      "Acc :  0.714622641509434\n",
      "len all :  424\n",
      "Qwk original :  0.7829207032651596\n",
      "Acc original :  0.6705882352941176\n",
      "len ori :  340\n",
      "Acc off topic :  0.8928571428571429\n",
      "len off :  84\n",
      "\n",
      "Mean QWK :  0.827861764410455\n",
      "\n",
      "Mean QWK Original :  0.7696616761150693\n",
      "\n",
      "Mean Accuracy :  0.7068890122086571\n",
      "\n",
      "Mean Accuracy Original :  0.6658624166466589\n",
      "\n",
      "Mean Accuracy Off Topic :  0.9156245188133733\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "qwk_scores_ori = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_ori = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_ori = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_ori = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x_off, y_off):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x_off[train_index], x_off[test_index], y_off[train_index], y_off[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ALL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "\n",
    "    # PREDICT AND EVALUATE ONLY ORIGINAL ESSAY\n",
    "    test_index_ori = [a for a in test_index if a < 1772]\n",
    "    x_test_ori = x_off[test_index_ori]\n",
    "    y_test_ori = y_off[test_index_ori]\n",
    "    predict_ori = model2.predict(x_test_ori)\n",
    "    predict_ori = np.round(predict_ori)\n",
    "    pred_labels_ori.extend(predict_ori)\n",
    "    \n",
    "    result_qwk_ori = quadratic_weighted_kappa(y_test_ori, predict_ori)\n",
    "    print(\"Qwk original : \", result_qwk_ori)\n",
    "    qwk_scores_ori.append(result_qwk_ori)\n",
    "    \n",
    "    result_acc_ori = accuracy_score(y_test_ori, predict_ori)\n",
    "    print(\"Acc original : \", result_acc_ori)\n",
    "    acc_scores_ori.append(result_acc_ori)\n",
    "    \n",
    "    print(\"len ori : \", len(test_index_ori))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY OFF-TOPIC ESSAY\n",
    "    test_index_off = [a for a in test_index if a > 1771]\n",
    "    x_test_off = x_off[test_index_off]\n",
    "    y_test_off = y_off[test_index_off]\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(test_index_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Original : \", np.mean(qwk_scores_ori))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Original : \", np.mean(acc_scores_ori))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3.0: 200, 1.0: 746, 2.0: 592, 0.0: 234})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_labels_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 320, 1.0: 30})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(pred_labels_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original data (1772 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.7889320832380516\n",
      "Acc :  0.6873239436619718\n",
      "len all :  355\n",
      "Acc off topic :  0.05714285714285714\n",
      "len off :  350\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.7567900610151411\n",
      "Acc :  0.6450704225352113\n",
      "len all :  355\n",
      "Acc off topic :  0.03142857142857143\n",
      "len off :  350\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.7580874081394493\n",
      "Acc :  0.6666666666666666\n",
      "len all :  354\n",
      "Acc off topic :  0.05142857142857143\n",
      "len off :  350\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.7500549157435592\n",
      "Acc :  0.652542372881356\n",
      "len all :  354\n",
      "Acc off topic :  0.04285714285714286\n",
      "len off :  350\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.8139692013175305\n",
      "Acc :  0.692090395480226\n",
      "len all :  354\n",
      "Acc off topic :  0.03428571428571429\n",
      "len off :  350\n",
      "\n",
      "Mean QWK :  0.7735667338907464\n",
      "\n",
      "Mean Accuracy :  0.6687387602450864\n",
      "\n",
      "Mean Accuracy Off Topic :  0.043428571428571434\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ORIGINAL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY offBERISH ESSAY\n",
    "    x_test_off = off\n",
    "    y_test_off = np.zeros(350)\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(x_test_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 746, 2.0: 592, 0.0: 234, 3.0: 200})\n",
      "Counter({2.0: 1052, 1.0: 597, 0.0: 76, 3.0: 25})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))\n",
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_asap6_extended_780_normalized')\n",
    "\n",
    "d_off = xgboost.DMatrix(off, feature_names=feature_names)\n",
    "pred = model.predict(d_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 3., 1., 2., 1., 1., 1., 2., 3., 2., 2., 1., 2., 2., 1.,\n",
       "       2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 3., 1., 3., 2., 0., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 3., 2., 3., 2., 1., 2., 3., 2., 2., 2., 2., 3., 1., 2., 2., 3.,\n",
       "       1., 3., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 1., 1., 3., 2., 2., 3., 3., 2., 1., 2., 3.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 3., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 2., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 1., 2., 1., 2., 2., 2.,\n",
       "       2., 1., 1., 2., 1., 2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.round(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 169, 2.0: 142, 3.0: 18, 0.0: 21})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 54.29%\n"
     ]
    }
   ],
   "source": [
    "pred_failed = [a for a in pred if a < 2]\n",
    "acc = (len(pred_failed) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 6.00%\n"
     ]
    }
   ],
   "source": [
    "pred_zero = [a for a in pred if a == 0]\n",
    "acc = (len(pred_zero) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
