{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('essay_ease10_sbert768_simbow_langerr_780_normalized_asap7')\n",
    "x_off = joblib.load('essay_asap7_780_with350offtopic')\n",
    "y = joblib.load('score_asap7')\n",
    "y_off = joblib.load('score_asap7_with350offtopic')\n",
    "off = joblib.load('essay_350_offtopic_780_except7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1569, 780)\n",
      "(1919, 780)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1569,)\n",
      "(1919,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 780)\n"
     ]
    }
   ],
   "source": [
    "print(off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len feature names:  780\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names_extended():\n",
    "    ease_feats = ['Answer Length', 'Word Counts', 'Average Word Length', 'Good n-gram', 'Prompt Overlap',\n",
    "              'Prompt Overlap (synonyms)', 'Punctuation Counts', 'Spelling Error', 'Unique Words', 'Prompt Similarity SBert']\n",
    "\n",
    "    sbert_feats = []\n",
    "    sbert_dim = 768\n",
    "\n",
    "    for i in range(0, sbert_dim):\n",
    "    \tfname = \"sbert_\" + str(i) \n",
    "    \tsbert_feats.append(fname)\n",
    "    \n",
    "    prompt_similarity_bow = [\"Prompt Similarity BOW\"]\n",
    "    lang_error = [\"Language Error\"]\n",
    "    \n",
    "    feature_names = ease_feats + prompt_similarity_bow + lang_error + sbert_feats \n",
    "\n",
    "    print(\"len feature names: \", len(feature_names))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names_extended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
    "                colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.03,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original + off topic data (1919 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.9016426144410878\n",
      "Acc :  0.171875\n",
      "len all :  384\n",
      "Qwk original :  0.7524821832400828\n",
      "Acc original :  0.14873417721518986\n",
      "len ori :  316\n",
      "Acc off topic :  0.27941176470588236\n",
      "len off :  68\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.9043371237849961\n",
      "Acc :  0.1640625\n",
      "len all :  384\n",
      "Qwk original :  0.7440852275095724\n",
      "Acc original :  0.1619047619047619\n",
      "len ori :  315\n",
      "Acc off topic :  0.17391304347826086\n",
      "len off :  69\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.895320334589317\n",
      "Acc :  0.15364583333333334\n",
      "len all :  384\n",
      "Qwk original :  0.7236140537430722\n",
      "Acc original :  0.14826498422712933\n",
      "len ori :  317\n",
      "Acc off topic :  0.1791044776119403\n",
      "len off :  67\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.9038776389059072\n",
      "Acc :  0.16145833333333334\n",
      "len all :  384\n",
      "Qwk original :  0.7342747931717638\n",
      "Acc original :  0.1536050156739812\n",
      "len ori :  319\n",
      "Acc off topic :  0.2\n",
      "len off :  65\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.8856273988588923\n",
      "Acc :  0.16449086161879894\n",
      "len all :  383\n",
      "Qwk original :  0.657827529032262\n",
      "Acc original :  0.1490066225165563\n",
      "len ori :  302\n",
      "Acc off topic :  0.2222222222222222\n",
      "len off :  81\n",
      "\n",
      "Mean QWK :  0.8981610221160402\n",
      "\n",
      "Mean QWK Original :  0.7224567573393507\n",
      "\n",
      "Mean Accuracy :  0.16310650565709314\n",
      "\n",
      "Mean Accuracy Original :  0.15230311230752372\n",
      "\n",
      "Mean Accuracy Off Topic :  0.21093030160366116\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "qwk_scores_ori = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_ori = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_ori = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_ori = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x_off, y_off):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x_off[train_index], x_off[test_index], y_off[train_index], y_off[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ALL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "\n",
    "    # PREDICT AND EVALUATE ONLY ORIGINAL ESSAY\n",
    "    test_index_ori = [a for a in test_index if a < 1569]\n",
    "    x_test_ori = x_off[test_index_ori]\n",
    "    y_test_ori = y_off[test_index_ori]\n",
    "    predict_ori = model2.predict(x_test_ori)\n",
    "    predict_ori = np.round(predict_ori)\n",
    "    pred_labels_ori.extend(predict_ori)\n",
    "    \n",
    "    result_qwk_ori = quadratic_weighted_kappa(y_test_ori, predict_ori)\n",
    "    print(\"Qwk original : \", result_qwk_ori)\n",
    "    qwk_scores_ori.append(result_qwk_ori)\n",
    "    \n",
    "    result_acc_ori = accuracy_score(y_test_ori, predict_ori)\n",
    "    print(\"Acc original : \", result_acc_ori)\n",
    "    acc_scores_ori.append(result_acc_ori)\n",
    "    \n",
    "    print(\"len ori : \", len(test_index_ori))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY OFF-TOPIC ESSAY\n",
    "    test_index_off = [a for a in test_index if a > 1568]\n",
    "    x_test_off = x_off[test_index_off]\n",
    "    y_test_off = y_off[test_index_off]\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(test_index_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Original : \", np.mean(qwk_scores_ori))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Original : \", np.mean(acc_scores_ori))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also check for minus score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({17.0: 166, 16.0: 153, 18.0: 150, 20.0: 132, 14.0: 132, 15.0: 130, 19.0: 124, 13.0: 118, 12.0: 109, 21.0: 81, 11.0: 70, 10.0: 51, 9.0: 47, 22.0: 45, 7.0: 19, 8.0: 18, 23.0: 14, 6.0: 4, 3.0: 2, 24.0: 2, 5.0: 1, 4.0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-0.0: 74, -1.0: 57, 1.0: 55, 3.0: 36, 2.0: 35, -2.0: 22, 4.0: 17, 5.0: 14, 7.0: 9, 9.0: 6, 8.0: 5, -3.0: 4, 6.0: 4, 11.0: 4, -4.0: 3, 12.0: 2, 20.0: 1, 10.0: 1, 15.0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i < 4 for i in pred_labels_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy Off Topic :  0.8171428571428572\n"
     ]
    }
   ],
   "source": [
    "# SO the Accuracies is 160 / 350\n",
    "print(\"\\nMean Accuracy Off Topic : \", sum(i < 4 for i in pred_labels_off) / len(pred_labels_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original data (1569 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.7931416123535302\n",
      "Acc :  0.16560509554140126\n",
      "len all :  314\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.7780216629342351\n",
      "Acc :  0.15605095541401273\n",
      "len all :  314\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.8062406348636821\n",
      "Acc :  0.14012738853503184\n",
      "len all :  314\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.7932437700883994\n",
      "Acc :  0.16878980891719744\n",
      "len all :  314\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.7149977236239904\n",
      "Acc :  0.1182108626198083\n",
      "len all :  313\n",
      "Acc off topic :  0.0\n",
      "len off :  350\n",
      "\n",
      "Mean QWK :  0.7771290807727675\n",
      "\n",
      "Mean Accuracy :  0.1497568222054903\n",
      "\n",
      "Mean Accuracy Off Topic :  0.0\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ORIGINAL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY offBERISH ESSAY\n",
    "    x_test_off = off\n",
    "    y_test_off = np.zeros(350)\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(x_test_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({17.0: 166, 16.0: 153, 18.0: 150, 20.0: 132, 14.0: 132, 15.0: 130, 19.0: 124, 13.0: 118, 12.0: 109, 21.0: 81, 11.0: 70, 10.0: 51, 9.0: 47, 22.0: 45, 7.0: 19, 8.0: 18, 23.0: 14, 6.0: 4, 3.0: 2, 24.0: 2, 5.0: 1, 4.0: 1})\n",
      "Counter({16.0: 240, 18.0: 217, 17.0: 216, 15.0: 197, 14.0: 144, 19.0: 142, 13.0: 111, 20.0: 106, 12.0: 66, 22.0: 64, 23.0: 61, 21.0: 52, 11.0: 46, 10.0: 41, 24.0: 24, 9.0: 9, 25.0: 7, 8.0: 6, 7.0: 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))\n",
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_asap6_extended_780_normalized')\n",
    "\n",
    "d_off = xgboost.DMatrix(off, feature_names=feature_names)\n",
    "pred = model.predict(d_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 3., 1., 2., 1., 1., 1., 2., 3., 2., 2., 1., 2., 2., 1.,\n",
       "       2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 3., 1., 3., 2., 0., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 3., 2., 3., 2., 1., 2., 3., 2., 2., 2., 2., 3., 1., 2., 2., 3.,\n",
       "       1., 3., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 1., 1., 3., 2., 2., 3., 3., 2., 1., 2., 3.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 3., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 2., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 1., 2., 1., 2., 2., 2.,\n",
       "       2., 1., 1., 2., 1., 2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.round(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 169, 2.0: 142, 3.0: 18, 0.0: 21})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 54.29%\n"
     ]
    }
   ],
   "source": [
    "pred_failed = [a for a in pred if a < 2]\n",
    "acc = (len(pred_failed) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 6.00%\n"
     ]
    }
   ],
   "source": [
    "pred_zero = [a for a in pred if a == 0]\n",
    "acc = (len(pred_zero) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
