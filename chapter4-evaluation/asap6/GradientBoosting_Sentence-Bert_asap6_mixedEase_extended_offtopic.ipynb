{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import joblib\n",
    "import numpy as np\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = joblib.load('essay_ease10_sbert768_simbow_langerr_780_normalized_asap6')\n",
    "x_off = joblib.load('essay_asap6_780_with350offtopic')\n",
    "y = joblib.load('score_asap6')\n",
    "y_off = joblib.load('score_asap6_with350offtopic')\n",
    "off = joblib.load('essay_350_offtopic_780')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 780)\n",
      "(2150, 780)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800,)\n",
      "(2150,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y_off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 780)\n"
     ]
    }
   ],
   "source": [
    "print(off.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len feature names:  780\n"
     ]
    }
   ],
   "source": [
    "def get_feature_names_extended():\n",
    "    ease_feats = ['Answer Length', 'Word Counts', 'Average Word Length', 'Good n-gram', 'Prompt Overlap',\n",
    "              'Prompt Overlap (synonyms)', 'Punctuation Counts', 'Spelling Error', 'Unique Words', 'Prompt Similarity SBert']\n",
    "\n",
    "    sbert_feats = []\n",
    "    sbert_dim = 768\n",
    "\n",
    "    for i in range(0, sbert_dim):\n",
    "    \tfname = \"sbert_\" + str(i) \n",
    "    \tsbert_feats.append(fname)\n",
    "    \n",
    "    prompt_similarity_bow = [\"Prompt Similarity BOW\"]\n",
    "    lang_error = [\"Language Error\"]\n",
    "    \n",
    "    feature_names = ease_feats + prompt_similarity_bow + lang_error + sbert_feats \n",
    "\n",
    "    print(\"len feature names: \", len(feature_names))\n",
    "    \n",
    "    return feature_names\n",
    "\n",
    "feature_names = get_feature_names_extended()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 5-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=42, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = xgboost.XGBRegressor(objective ='reg:squarederror',\n",
    "                colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.05,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original + off topic data (2150 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.9111760667724871\n",
      "Acc :  0.7046511627906977\n",
      "len all :  430\n",
      "Qwk original :  0.7892781364683599\n",
      "Acc original :  0.651685393258427\n",
      "len ori :  356\n",
      "Acc off topic :  0.9594594594594594\n",
      "len off :  74\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.9032109617545744\n",
      "Acc :  0.7\n",
      "len all :  430\n",
      "Qwk original :  0.7650147450190606\n",
      "Acc original :  0.6446280991735537\n",
      "len ori :  363\n",
      "Acc off topic :  1.0\n",
      "len off :  67\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.9095905832491764\n",
      "Acc :  0.7255813953488373\n",
      "len all :  430\n",
      "Qwk original :  0.7995895330938944\n",
      "Acc original :  0.6854838709677419\n",
      "len ori :  372\n",
      "Acc off topic :  0.9827586206896551\n",
      "len off :  58\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.9095073037880664\n",
      "Acc :  0.7232558139534884\n",
      "len all :  430\n",
      "Qwk original :  0.8006529002201838\n",
      "Acc original :  0.6815642458100558\n",
      "len ori :  358\n",
      "Acc off topic :  0.9305555555555556\n",
      "len off :  72\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.9129668729523116\n",
      "Acc :  0.6906976744186046\n",
      "len all :  430\n",
      "Qwk original :  0.7822639372117486\n",
      "Acc original :  0.6239316239316239\n",
      "len ori :  351\n",
      "Acc off topic :  0.9873417721518988\n",
      "len off :  79\n",
      "\n",
      "Mean QWK :  0.9092903577033231\n",
      "\n",
      "Mean QWK Original :  0.7873598504026494\n",
      "\n",
      "Mean QWK Off Topic :  nan\n",
      "\n",
      "Mean Accuracy :  0.7088372093023256\n",
      "\n",
      "Mean Accuracy Original :  0.6574586466282805\n",
      "\n",
      "Mean Accuracy Off Topic :  0.9720230815713137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\20167947\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\20167947\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "qwk_scores_ori = []\n",
    "qwk_scores_off = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_ori = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_ori = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_ori = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x_off, y_off):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x_off[train_index], x_off[test_index], y_off[train_index], y_off[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)\n",
    "    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ALL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "\n",
    "    # PREDICT AND EVALUATE ONLY ORIGINAL ESSAY\n",
    "    test_index_ori = [a for a in test_index if a < 1800]\n",
    "    x_test_ori = x_off[test_index_ori]\n",
    "    y_test_ori = y_off[test_index_ori]\n",
    "    predict_ori = model2.predict(x_test_ori)\n",
    "    predict_ori = np.round(predict_ori)\n",
    "    pred_labels_ori.extend(predict_ori)\n",
    "    \n",
    "    result_qwk_ori = quadratic_weighted_kappa(y_test_ori, predict_ori)\n",
    "    print(\"Qwk original : \", result_qwk_ori)\n",
    "    qwk_scores_ori.append(result_qwk_ori)\n",
    "    \n",
    "    result_acc_ori = accuracy_score(y_test_ori, predict_ori)\n",
    "    print(\"Acc original : \", result_acc_ori)\n",
    "    acc_scores_ori.append(result_acc_ori)\n",
    "    \n",
    "    print(\"len ori : \", len(test_index_ori))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY GIBBERISH ESSAY\n",
    "    test_index_off = [a for a in test_index if a > 1799]\n",
    "    x_test_off = x_off[test_index_off]\n",
    "    y_test_off = y_off[test_index_off]\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(test_index_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Original : \", np.mean(qwk_scores_ori))\n",
    "print(\"\\nMean QWK Off Topic : \", np.mean(qwk_scores_off))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Original : \", np.mean(acc_scores_ori))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### also check for minus score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3.0: 837, 2.0: 466, 4.0: 324, 1.0: 157, 0.0: 16})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({-0.0: 340, 1.0: 10})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i < 1 for i in pred_labels_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Accuracy Off Topic :  0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "# SO the Accuracies is 340 / 350\n",
    "print(\"\\nMean Accuracy Off Topic : \", sum(i < 1 for i in pred_labels_off) / len(pred_labels_off))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training using original data (1800 essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loop - 1\n",
      "========\n",
      "Qwk :  0.8260045489006823\n",
      "Acc :  0.7166666666666667\n",
      "len all :  360\n",
      "Acc off topic :  0.08\n",
      "len off :  350\n",
      "\n",
      "Loop - 2\n",
      "========\n",
      "Qwk :  0.7988882527839849\n",
      "Acc :  0.6972222222222222\n",
      "len all :  360\n",
      "Acc off topic :  0.07428571428571429\n",
      "len off :  350\n",
      "\n",
      "Loop - 3\n",
      "========\n",
      "Qwk :  0.8027769012306721\n",
      "Acc :  0.6611111111111111\n",
      "len all :  360\n",
      "Acc off topic :  0.014285714285714285\n",
      "len off :  350\n",
      "\n",
      "Loop - 4\n",
      "========\n",
      "Qwk :  0.7795355099293168\n",
      "Acc :  0.6666666666666666\n",
      "len all :  360\n",
      "Acc off topic :  0.05714285714285714\n",
      "len off :  350\n",
      "\n",
      "Loop - 5\n",
      "========\n",
      "Qwk :  0.7854984894259819\n",
      "Acc :  0.6222222222222222\n",
      "len all :  360\n",
      "Acc off topic :  0.06285714285714286\n",
      "len off :  350\n",
      "\n",
      "Mean QWK :  0.7985407404541276\n",
      "\n",
      "Mean QWK Off Topic :  nan\n",
      "\n",
      "Mean Accuracy :  0.6727777777777777\n",
      "\n",
      "Mean Accuracy Off Topic :  0.05771428571428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\20167947\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\20167947\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "qwk_scores = []\n",
    "\n",
    "acc_scores = []\n",
    "acc_scores_off = []\n",
    "\n",
    "test_indices = []\n",
    "test_indices_off = []\n",
    "\n",
    "pred_labels = []\n",
    "pred_labels_off = []\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    print()\n",
    "    print(\"Loop -\", counter)\n",
    "    print(\"========\")\n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "         \n",
    "    model2.fit(X_train, Y_train)    \n",
    "    \n",
    "    # PREDICT AND EVALUATE ORIGINAL ESSAYS\n",
    "    predict = model2.predict(X_test)\n",
    "    predict = np.round(predict)\n",
    "    \n",
    "    pred_labels.extend(predict)\n",
    "    test_indices.extend(test_index)\n",
    "    \n",
    "    result_qwk = quadratic_weighted_kappa(Y_test, predict)\n",
    "    print(\"Qwk : \", result_qwk)\n",
    "    qwk_scores.append(result_qwk)\n",
    "    \n",
    "    result_acc = accuracy_score(Y_test, predict)\n",
    "    print(\"Acc : \", result_acc)\n",
    "    acc_scores.append(result_acc)\n",
    "    \n",
    "    print(\"len all : \", len(test_index))\n",
    "    \n",
    "    # PREDICT AND EVALUATE ONLY offBERISH ESSAY\n",
    "    x_test_off = off\n",
    "    y_test_off = np.zeros(350)\n",
    "    predict_off = model2.predict(x_test_off)\n",
    "    predict_off = np.round(predict_off)\n",
    "    pred_labels_off.extend(predict_off)\n",
    "    \n",
    "    result_acc_off = accuracy_score(y_test_off, predict_off)\n",
    "    print(\"Acc off topic : \", result_acc_off)\n",
    "    acc_scores_off.append(result_acc_off)\n",
    "    \n",
    "    print(\"len off : \", len(x_test_off))\n",
    "\n",
    "print(\"\\nMean QWK : \", np.mean(qwk_scores))\n",
    "print(\"\\nMean QWK Off Topic : \", np.mean(qwk_scores_off))\n",
    "\n",
    "print(\"\\nMean Accuracy : \", np.mean(acc_scores))\n",
    "print(\"\\nMean Accuracy Off Topic : \", np.mean(acc_scores_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3.0: 837, 2.0: 466, 4.0: 324, 1.0: 157, 0.0: 16})\n",
      "Counter({1.0: 788, 2.0: 762, 0.0: 101, 3.0: 99})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(pred_labels_ori))\n",
    "print(Counter(pred_labels_off))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('model_asap6_extended_780_normalized')\n",
    "\n",
    "d_off = xgboost.DMatrix(off, feature_names=feature_names)\n",
    "pred = model.predict(d_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., 2., 2., 1., 1., 2., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 3., 1., 2., 1., 1., 1., 2., 3., 2., 2., 1., 2., 2., 1.,\n",
       "       2., 2., 1., 1., 1., 2., 2., 1., 2., 2., 1., 2., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 3., 1., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n",
       "       2., 3., 1., 3., 2., 0., 1., 2., 2., 2., 2., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 3., 2., 3., 2., 1., 2., 3., 2., 2., 2., 2., 3., 1., 2., 2., 3.,\n",
       "       1., 3., 2., 2., 2., 1., 2., 1., 2., 1., 1., 2., 2., 1., 2., 2., 1.,\n",
       "       1., 2., 2., 2., 1., 1., 1., 1., 3., 2., 2., 3., 3., 2., 1., 2., 3.,\n",
       "       2., 1., 2., 1., 2., 2., 2., 2., 1., 2., 1., 2., 1., 3., 1., 1., 1.,\n",
       "       2., 0., 0., 1., 2., 1., 2., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 2., 1., 0.,\n",
       "       1., 0., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 0., 2.,\n",
       "       2., 1., 1., 1., 1., 2., 1., 1., 0., 1., 1., 0., 0., 2., 1., 1., 1.,\n",
       "       1., 0., 2., 1., 1., 1., 1., 2., 2., 1., 1., 1., 1., 1., 1., 1., 2.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 2., 1., 1., 1., 1., 1., 0., 1., 2., 1.,\n",
       "       1., 1., 2., 1., 0., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 0.,\n",
       "       2., 1., 1., 1., 2., 0., 1., 1., 1., 2., 1., 1., 1., 1., 2., 2., 1.,\n",
       "       2., 2., 2., 1., 2., 1., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 1., 2., 1., 2., 2., 2.,\n",
       "       2., 1., 1., 2., 1., 2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.round(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 169, 2.0: 142, 3.0: 18, 0.0: 21})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 54.29%\n"
     ]
    }
   ],
   "source": [
    "pred_failed = [a for a in pred if a < 2]\n",
    "acc = (len(pred_failed) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 6.00%\n"
     ]
    }
   ],
   "source": [
    "pred_zero = [a for a in pred if a == 0]\n",
    "acc = (len(pred_zero) / len(pred)) * 100\n",
    "print('Acc {:.2f}%'.format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
