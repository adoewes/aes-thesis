{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e36a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f32b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_set_7_final.txt', sep='\\t', error_bad_lines=False, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3982401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_cleantext_tokenizer(text, remove_nonletters = False, remove_stopwords=False, stemming=False, lemma=False):\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    teks = BeautifulSoup(text, 'lxml').get_text()\n",
    "\n",
    "    # 2. Remove non-ASCII\n",
    "    letters_only = re.sub(r\"[^\\x00-\\x7F]+\", \" \", teks)\n",
    "\n",
    "    if remove_nonletters:\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", letters_only)\n",
    "\n",
    "    #letters_only = teks\n",
    "\n",
    "    # 3. Convert to lower-case, split into words\n",
    "    words = letters_only.lower()\n",
    "    words = word_tokenize(words)\n",
    "\n",
    "    # 4. Convert stopwords into Set (faster than List)\n",
    "    # 5. Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 6. Stemming\n",
    "    if stemming:\n",
    "        porter = PorterStemmer()\n",
    "        stems = []\n",
    "        for t in words:\n",
    "            stems.append(porter.stem(t))\n",
    "        words = stems\n",
    "\n",
    "    # 7. Stemming\n",
    "    if lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for t in words:\n",
    "            lemmas.append(lemmatizer.lemmatize(t))\n",
    "        words = lemmas\n",
    "        \n",
    "    # 8. Join words back into one string by space, and return the result\n",
    "    return(\" \".join(words))\n",
    "    # 8. Return list of words\n",
    "    # return(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c5b5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Patience is when your waiting .I was patience when in line waiting for lunch .I didn\\x92t c ut any one to eat .I was standing and waiting for my turn .Patience ,some people don\\x92t have it .Lots of people just cut or yell at you because they don\\x92t have  any patience. Sometimes people will push you out of their way .They only do that because they don\\x92t have patience at all. Patience is what people need .People need patience because lots o f feelings get hurt .Everyone should have patience.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "594421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():  \n",
    "    line = df.at[idx, 'essay']\n",
    "    line = line.encode('ascii','ignore')\n",
    "    line = text_to_cleantext_tokenizer(line, remove_nonletters=True, remove_stopwords=True, lemma=True)\n",
    "    df.at[idx,'essay'] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9154d8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'patience waiting patience line waiting lunch didnt c ut one eat standing waiting turn patience people dont lot people cut yell dont patience sometimes people push way dont patience patience people need people need patience lot f feeling get hurt everyone patience'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4018db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa1bc8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8317"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee48e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'able get', ..., 'zoo', 'zoom', 'zoomed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "df8f1fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0b4613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0651608]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[625],X[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98225891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03732957]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[59],X[1064])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efc61319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2975554]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[756],X[270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d4a9974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[262],X[466])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "690b2632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essay_tfidf_no_stopword_asap7']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X.toarray(), 'essay_tfidf_no_stopword_asap7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b994cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "X = vect.fit_transform(['Patience is important'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da1ba863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "366e8dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patience': 3, 'is': 1, 'important': 0, 'patience is': 4, 'is important': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c8607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
