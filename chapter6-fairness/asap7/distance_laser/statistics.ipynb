{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from quadratic_weighted_kappa import quadratic_weighted_kappa\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv('../training_set_rel3.tsv', sep='\\t', encoding='ISO-8859-1')\n",
    "y =  joblib.load('score_asap7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = scores[scores['essay_set'] == 7]\n",
    "scores = scores[['rater1_domain1', 'rater2_domain1', 'rater3_domain1']]\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting : Quantitative Assessment of AES Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = joblib.load('score_model_gb_normalized_float_laser_whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK Score:  0.767266817272541\n",
      "Human Agreement:  0.7214784742548883\n",
      "Degradation:  -0.045788343017652666\n",
      "Z :  0.0001491376728501522\n",
      "Pearson correlation: 0.805\n",
      "P-value: 0.000\n",
      "Adjacent accuracy: 80.82%\n",
      "Exact accuracy: 13.70%\n",
      "RMSE: 2.728\n"
     ]
    }
   ],
   "source": [
    "qwk_model = quadratic_weighted_kappa(y, model_score)\n",
    "print(\"QWK Score: \", qwk_model)\n",
    "\n",
    "qwk_human = quadratic_weighted_kappa(scores['rater1_domain1'], scores['rater2_domain1'])\n",
    "print(\"Human Agreement: \", qwk_human)\n",
    "\n",
    "print(\"Degradation: \", qwk_human - qwk_model)\n",
    "\n",
    "rater1_score = scores['rater1_domain1']\n",
    "rater2_score = scores['rater2_domain1']\n",
    "\n",
    "human1_score = np.array(rater1_score)\n",
    "human2_score = np.array(rater2_score)\n",
    "\n",
    "model_mean = np.mean(model_score)\n",
    "#print(model_mean)\n",
    "y_mean = np.mean(y)\n",
    "#print(y_mean)\n",
    "mean_diff = abs(model_mean-y_mean)\n",
    "#print(mean_diff)\n",
    "\n",
    "model_variance = np.var(model_score)\n",
    "#print(model_variance)\n",
    "y_variance = np.var(y)\n",
    "#print(y_variance)\n",
    "\n",
    "z = mean_diff / np.sqrt((model_variance + y_variance)/2)\n",
    "print(\"Z : \",z)\n",
    "\n",
    "# Compute Pearson correlation\n",
    "corr, p_value = pearsonr(y, model_score)\n",
    "\n",
    "print(f\"Pearson correlation: {corr:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Round the model predictions\n",
    "model_score_rounded = np.rint(model_score).astype(int)\n",
    "\n",
    "# Compute absolute errors for adjacent accuracy\n",
    "errors = np.abs(y - model_score_rounded)\n",
    "adjacent = np.sum(errors <= 3)\n",
    "adjacent_accuracy = adjacent / len(y)\n",
    "print(f'Adjacent accuracy: {adjacent_accuracy:.2%}')\n",
    "\n",
    "# Compute exact accuracy\n",
    "exact_matches = np.sum(y == model_score_rounded)\n",
    "exact_accuracy = exact_matches / len(y)\n",
    "print(f'Exact accuracy: {exact_accuracy:.2%}')\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y, model_score)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest : Quantitative Assessment of AES Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = joblib.load('score_model_rf_normalized_float_laser_whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK Score:  0.7128991492478383\n",
      "Human Agreement:  0.7214784742548883\n",
      "Degradation:  0.008579325007050054\n",
      "Z :  0.006820330767998374\n",
      "Pearson correlation: 0.777\n",
      "P-value: 0.000\n",
      "Adjacent accuracy: 77.82%\n",
      "Exact accuracy: 14.60%\n",
      "RMSE: 2.923\n"
     ]
    }
   ],
   "source": [
    "qwk_model = quadratic_weighted_kappa(y, model_score)\n",
    "print(\"QWK Score: \", qwk_model)\n",
    "\n",
    "qwk_human = quadratic_weighted_kappa(scores['rater1_domain1'], scores['rater2_domain1'])\n",
    "print(\"Human Agreement: \", qwk_human)\n",
    "\n",
    "print(\"Degradation: \", qwk_human - qwk_model)\n",
    "\n",
    "rater1_score = scores['rater1_domain1']\n",
    "rater2_score = scores['rater2_domain1']\n",
    "\n",
    "human1_score = np.array(rater1_score)\n",
    "human2_score = np.array(rater2_score)\n",
    "\n",
    "model_mean = np.mean(model_score)\n",
    "#print(model_mean)\n",
    "y_mean = np.mean(y)\n",
    "#print(y_mean)\n",
    "mean_diff = abs(model_mean-y_mean)\n",
    "#print(mean_diff)\n",
    "\n",
    "model_variance = np.var(model_score)\n",
    "#print(model_variance)\n",
    "y_variance = np.var(y)\n",
    "#print(y_variance)\n",
    "\n",
    "z = mean_diff / np.sqrt((model_variance + y_variance)/2)\n",
    "print(\"Z : \",z)\n",
    "\n",
    "# Compute Pearson correlation\n",
    "corr, p_value = pearsonr(y, model_score)\n",
    "\n",
    "print(f\"Pearson correlation: {corr:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Round the model predictions\n",
    "model_score_rounded = np.rint(model_score).astype(int)\n",
    "\n",
    "# Compute absolute errors for adjacent accuracy\n",
    "errors = np.abs(y - model_score_rounded)\n",
    "adjacent = np.sum(errors <= 3)\n",
    "adjacent_accuracy = adjacent / len(y)\n",
    "print(f'Adjacent accuracy: {adjacent_accuracy:.2%}')\n",
    "\n",
    "# Compute exact accuracy\n",
    "exact_matches = np.sum(y == model_score_rounded)\n",
    "exact_accuracy = exact_matches / len(y)\n",
    "print(f'Exact accuracy: {exact_accuracy:.2%}')\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y, model_score)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression : Quantitative Assessment of AES Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = joblib.load('score_model_rr_normalized_float_laser_whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK Score:  0.7579343835000356\n",
      "Human Agreement:  0.7214784742548883\n",
      "Degradation:  -0.0364559092451473\n",
      "Z :  0.002380282391673543\n",
      "Pearson correlation: 0.781\n",
      "P-value: 0.000\n",
      "Adjacent accuracy: 78.78%\n",
      "Exact accuracy: 14.09%\n",
      "RMSE: 2.869\n"
     ]
    }
   ],
   "source": [
    "qwk_model = quadratic_weighted_kappa(y, model_score)\n",
    "print(\"QWK Score: \", qwk_model)\n",
    "\n",
    "qwk_human = quadratic_weighted_kappa(scores['rater1_domain1'], scores['rater2_domain1'])\n",
    "print(\"Human Agreement: \", qwk_human)\n",
    "\n",
    "print(\"Degradation: \", qwk_human - qwk_model)\n",
    "\n",
    "rater1_score = scores['rater1_domain1']\n",
    "rater2_score = scores['rater2_domain1']\n",
    "\n",
    "human1_score = np.array(rater1_score)\n",
    "human2_score = np.array(rater2_score)\n",
    "\n",
    "model_mean = np.mean(model_score)\n",
    "#print(model_mean)\n",
    "y_mean = np.mean(y)\n",
    "#print(y_mean)\n",
    "mean_diff = abs(model_mean-y_mean)\n",
    "#print(mean_diff)\n",
    "\n",
    "model_variance = np.var(model_score)\n",
    "#print(model_variance)\n",
    "y_variance = np.var(y)\n",
    "#print(y_variance)\n",
    "\n",
    "z = mean_diff / np.sqrt((model_variance + y_variance)/2)\n",
    "print(\"Z : \",z)\n",
    "\n",
    "# Compute Pearson correlation\n",
    "corr, p_value = pearsonr(y, model_score)\n",
    "\n",
    "print(f\"Pearson correlation: {corr:.3f}\")\n",
    "print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "# Round the model predictions\n",
    "model_score_rounded = np.rint(model_score).astype(int)\n",
    "\n",
    "# Compute absolute errors for adjacent accuracy\n",
    "errors = np.abs(y - model_score_rounded)\n",
    "adjacent = np.sum(errors <= 3)\n",
    "adjacent_accuracy = adjacent / len(y)\n",
    "print(f'Adjacent accuracy: {adjacent_accuracy:.2%}')\n",
    "\n",
    "# Compute exact accuracy\n",
    "exact_matches = np.sum(y == model_score_rounded)\n",
    "exact_accuracy = exact_matches / len(y)\n",
    "print(f'Exact accuracy: {exact_accuracy:.2%}')\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y, model_score)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'RMSE: {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
