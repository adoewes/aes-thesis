{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78e36a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "064f3dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\20167947\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('asap7_paraphrased.txt', sep=\" \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17825e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tolerance is the point at which your pausing ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm not a tolerance individual, similar to I c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day I was at b-ball practice and I was run...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I going to expound on when I went to the @ORGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It very well may be exceptionally difficult fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Once I was getting a cool @CAPS1 game it was s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>A patent individual in my life is my mother. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>At the point when another person I know showed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>I disdain weddings. I love when individuals ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>Half a month prior, we had a carport deal and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  essay\n",
       "0     Tolerance is the point at which your pausing ....\n",
       "1     I'm not a tolerance individual, similar to I c...\n",
       "2     One day I was at b-ball practice and I was run...\n",
       "3     I going to expound on when I went to the @ORGA...\n",
       "4     It very well may be exceptionally difficult fo...\n",
       "...                                                 ...\n",
       "1564  Once I was getting a cool @CAPS1 game it was s...\n",
       "1565  A patent individual in my life is my mother. A...\n",
       "1566  At the point when another person I know showed...\n",
       "1567  I disdain weddings. I love when individuals ge...\n",
       "1568  Half a month prior, we had a carport deal and ...\n",
       "\n",
       "[1569 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3982401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_cleantext_tokenizer(text, remove_nonletters = False, remove_stopwords=False, stemming=False, lemma=False):\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    teks = BeautifulSoup(text, 'lxml').get_text()\n",
    "\n",
    "    # 2. Remove non-ASCII\n",
    "    letters_only = re.sub(r\"[^\\x00-\\x7F]+\", \" \", teks)\n",
    "\n",
    "    if remove_nonletters:\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", letters_only)\n",
    "\n",
    "    #letters_only = teks\n",
    "\n",
    "    # 3. Convert to lower-case, split into words\n",
    "    words = letters_only.lower()\n",
    "    words = word_tokenize(words)\n",
    "\n",
    "    # 4. Convert stopwords into Set (faster than List)\n",
    "    # 5. Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 6. Stemming\n",
    "    if stemming:\n",
    "        porter = PorterStemmer()\n",
    "        stems = []\n",
    "        for t in words:\n",
    "            stems.append(porter.stem(t))\n",
    "        words = stems\n",
    "\n",
    "    # 7. Stemming\n",
    "    if lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for t in words:\n",
    "            lemmas.append(lemmatizer.lemmatize(t))\n",
    "        words = lemmas\n",
    "        \n",
    "    # 8. Join words back into one string by space, and return the result\n",
    "    return(\" \".join(words))\n",
    "    # 8. Return list of words\n",
    "    # return(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7c5b5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tolerance is the point at which your pausing .I was persistence when in line sitting tight for lunch .I didnæt c ut any one to eat .I was standing and sitting tight for my turn .Patience ,a few group donæt have it .Lots of individuals just cut or shout at you since they donæt have any persistence. Once in a while individuals will push you out of their way .They just do that since they donæt have tolerance by any stretch of the imagination. Persistence is the thing that individuals need .People need tolerance since parcels o f emotions get injured .Everyone ought to have tolerance.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "594421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():  \n",
    "    line = df.at[idx, 'essay']\n",
    "    line = line.encode('ascii','ignore')\n",
    "    line = text_to_cleantext_tokenizer(line, remove_nonletters=True, remove_stopwords=False, lemma=True)\n",
    "    df.at[idx,'essay'] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9154d8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tolerance is the point at which your pausing i wa persistence when in line sitting tight for lunch i didnt c ut any one to eat i wa standing and sitting tight for my turn patience a few group dont have it lot of individual just cut or shout at you since they dont have any persistence once in a while individual will push you out of their way they just do that since they dont have tolerance by any stretch of the imagination persistence is the thing that individual need people need tolerance since parcel o f emotion get injured everyone ought to have tolerance'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4018db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa1bc8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16139"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee48e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'ability that', 'ability to', ..., 'zoo', 'zoom',\n",
       "       'zoomed'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df8f1fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0b4613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45285465]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[625],X[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98225891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01365418]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[59],X[1064])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efc61319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58267886]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[756],X[270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d4a9974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26492824]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[262],X[466])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "690b2632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essay_ngram_paraphrase_asap7']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X.toarray(), 'essay_ngram_paraphrase_asap7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98b994cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "X = vect.fit_transform(['Patience is important'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da1ba863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "366e8dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patience': 3, 'is': 1, 'important': 0, 'patience is': 4, 'is important': 2}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095aa0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
