{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e36a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f32b374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asap7_paraphrased.txt', sep=\" \\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3982401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_cleantext_tokenizer(text, remove_nonletters = False, remove_stopwords=False, stemming=False, lemma=False):\n",
    "\n",
    "    # 1. Remove HTML\n",
    "    teks = BeautifulSoup(text, 'lxml').get_text()\n",
    "\n",
    "    # 2. Remove non-ASCII\n",
    "    letters_only = re.sub(r\"[^\\x00-\\x7F]+\", \" \", teks)\n",
    "\n",
    "    if remove_nonletters:\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", letters_only)\n",
    "\n",
    "    #letters_only = teks\n",
    "\n",
    "    # 3. Convert to lower-case, split into words\n",
    "    words = letters_only.lower()\n",
    "    words = word_tokenize(words)\n",
    "\n",
    "    # 4. Convert stopwords into Set (faster than List)\n",
    "    # 5. Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    # 6. Stemming\n",
    "    if stemming:\n",
    "        porter = PorterStemmer()\n",
    "        stems = []\n",
    "        for t in words:\n",
    "            stems.append(porter.stem(t))\n",
    "        words = stems\n",
    "\n",
    "    # 7. Stemming\n",
    "    if lemma:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for t in words:\n",
    "            lemmas.append(lemmatizer.lemmatize(t))\n",
    "        words = lemmas\n",
    "        \n",
    "    # 8. Join words back into one string by space, and return the result\n",
    "    return(\" \".join(words))\n",
    "    # 8. Return list of words\n",
    "    # return(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7c5b5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tolerance is the point at which your pausing .I was persistence when in line sitting tight for lunch .I didnæt c ut any one to eat .I was standing and sitting tight for my turn .Patience ,a few group donæt have it .Lots of individuals just cut or shout at you since they donæt have any persistence. Once in a while individuals will push you out of their way .They just do that since they donæt have tolerance by any stretch of the imagination. Persistence is the thing that individuals need .People need tolerance since parcels o f emotions get injured .Everyone ought to have tolerance.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594421df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():  \n",
    "    line = df.at[idx, 'essay']\n",
    "    line = line.encode('ascii','ignore')\n",
    "    line = text_to_cleantext_tokenizer(line, remove_nonletters=True, remove_stopwords=False, lemma=True)\n",
    "    df.at[idx,'essay'] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9154d8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tolerance is the point at which your pausing i wa persistence when in line sitting tight for lunch i didnt c ut any one to eat i wa standing and sitting tight for my turn patience a few group dont have it lot of individual just cut or shout at you since they dont have any persistence once in a while individual will push you out of their way they just do that since they dont have tolerance by any stretch of the imagination persistence is the thing that individual need people need tolerance since parcel o f emotion get injured everyone ought to have tolerance'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4018db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1bc8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16139"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee48e9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'ability that', 'ability to', ..., 'zoo', 'zoom',\n",
       "       'zoomed'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df8f1fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b4613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10755893]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[625],X[880])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98225891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00561058]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[59],X[1064])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc61319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22997721]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[756],X[270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d4a9974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03467353]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(X[262],X[466])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "690b2632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['essay_tfidf_paraphrase_asap7']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(X.toarray(), 'essay_tfidf_paraphrase_asap7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98b994cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "X = vect.fit_transform(['Patience is important'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da1ba863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "366e8dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'patience': 3, 'is': 1, 'important': 0, 'patience is': 4, 'is important': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c8607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
